{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#0,1問題は分類に当たる\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■■■■■　関数　■■■■■"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveFig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=600):\n",
    "   \n",
    "    images_path = os.path.join(\"..\", \"Images\")\n",
    "    os.makedirs(images_path, exist_ok=True)\n",
    "    file_name = os.path.join(images_path, fig_id + \".\" + fig_extension)\n",
    "    \n",
    "    #print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(file_name, format=fig_extension, dpi=resolution)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetIntervalCSVFiles(r_path):\n",
    "    files = os.listdir(r_path)\n",
    "    csv_files = []\n",
    "    \n",
    "    for file in files:\n",
    "        if(file.find(\".csv\") != -1 and file.upper().find(\"INTERVAL\") != -1):\n",
    "             csv_files.append(file)\n",
    "    \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetConcatSubAssyFiles(r_path):\n",
    "    files = os.listdir(r_path)\n",
    "    csv_files = []\n",
    "    \n",
    "    for file in files:\n",
    "        if(file.find(\".csv\") != -1 and file.upper().startswith(\"CONCAT\") and file.find(\"SA\") != -1):\n",
    "            csv_files.append(file)\n",
    "            \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetConcatAssyWithGelFiles(r_path):\n",
    "    files = os.listdir(r_path)\n",
    "    csv_files = []\n",
    "    \n",
    "    for file in files:\n",
    "        if(file.find(\".csv\") != -1 and file.upper().startswith(\"CONCAT\") and file.find(\"WithGel\") != -1):\n",
    "            csv_files.append(file)\n",
    "            \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetConcatAssyFiles(r_path):\n",
    "    files = os.listdir(r_path)\n",
    "    csv_files = []\n",
    "    \n",
    "    for file in files:\n",
    "        if (file.find(\".csv\") != -1 and file.upper().startswith(\"CONCAT\") and file.find(\"Assy\") != -1):\n",
    "            csv_files.append(file)\n",
    "            \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnderSampling(df,num,label) :\n",
    "\n",
    "    #label = 少数派のラベル, num = ターゲット件数\n",
    "    \n",
    "    time_cols = [\"ti_%s_to_%s[s]\" %(i, i+1) for i in np.arange(32,37)]\n",
    "    proc_cols = [\"base_height_L_a_32\", \"base_height_R_a_32\", \"gel_plunger_stroke_a_35\", \"base_height_a_35\", \"gel_thick_b4_a_35\"]\n",
    "    eval_cols = time_cols + proc_cols\n",
    "    obj_cols  = [\"defective_cat_37\"]\n",
    "    \n",
    "    X = df[eval_cols]\n",
    "    Y = df[obj_cols]\n",
    "    \n",
    "    # KMeansによるクラスタリング\n",
    "    km = KMeans(random_state=42)\n",
    "    km.fit(X,Y)\n",
    "    X[\"Cluster\"] = km.predict(X)\n",
    "\n",
    "    # 群別の構成比を少数派の件数に乗じて群別の抽出件数を計算\n",
    "    count_sum = X.groupby(\"Cluster\").count().iloc[0:,0].as_matrix()\n",
    "    ratio = count_sum / count_sum.sum()\n",
    "    samp_num = np.round(ratio * num,0).astype(np.int32)\n",
    "\n",
    "    # 群別にサンプリング処理を実施\n",
    "    for i in np.arange(8) :\n",
    "        tmp = X[X[\"Cluster\"]==i]\n",
    "        if i == 0 :\n",
    "            tmp1 = X.sample(samp_num[i],replace=True)\n",
    "        else :\n",
    "            tmp2 = X.sample(samp_num[i],replace=True)\n",
    "            tmp1 = pd.concat([tmp1,tmp2])\n",
    "    tmp1[\"Class\"] = label\n",
    "    \n",
    "    return tmp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#工程間　単体\n",
    "def PrepareFittingData(csv_file):\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    \n",
    "    #OK:0, NG:31のデータ抽出 query は　boolインデックスより新しいみたい\n",
    "    df = df.query(\"defective_cat_16 == 0 or defective_cat_16 == 31\") \n",
    "    \n",
    "    #X, Yとなるデータの抽出\n",
    "    cols = [\"ti_%s_to_%s[s]\" %(i, i+1) for i in np.arange(11,16)]\n",
    "    cols.extend([\"cure_time[s]\", \"defective_cat_16\"])\n",
    "    df = df[cols]\n",
    "    \n",
    "    #NG:31を1に変換\n",
    "    df.loc[df[\"defective_cat_16\"]==31, \"defective_cat_16\"] = 1\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ゲル気泡　数値データのみ\n",
    "def PrepareFittingData(csv_file):\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    \n",
    "    #OK:0, NG:31のデータ抽出 query は　boolインデックスより新しいみたい\n",
    "    df = df.query(\"defective_cat_37 == 0 or defective_cat_37 == 86\") \n",
    "    \n",
    "    #X, Yとなるデータの抽出\n",
    "    time_cols = [\"ti_%s_to_%s[s]\" %(i, i+1) for i in np.arange(32,37)]\n",
    "    proc_cols = [\"base_height_L_a_32\", \"base_height_R_a_32\", \"gel_plunger_stroke_a_35\", \"base_height_a_35\", \"gel_thick_b4_a_35\", \"defective_cat_37\"]\n",
    "    eval_cols = time_cols + proc_cols\n",
    "    \n",
    "    df = df[eval_cols]\n",
    "    \n",
    "    #NG:31を1に変換\n",
    "    df.loc[df[\"defective_cat_37\"]==86, \"defective_cat_37\"] = 1\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def CalcMethodComparison(csv_file):\n",
    "    \n",
    "    #定義\n",
    "    result = pd.DataFrame()\n",
    "    clf_method = [\"SGD\", \"Kernel\", \"LinearSVC\", \"KNeighbor\", \"SVC\", \"RandomForest\"]\n",
    "    conf_cols  = [\"TN\", \"FP\", \"FN\", \"TP\"]\n",
    "    method, precision, recall, f1, conf = [], [], [], [], []\n",
    "    \n",
    "    #データ準備\n",
    "    df = PrepareFittingData(csv_file)\n",
    "    \n",
    "    #データ数のバランス調整\n",
    "    df = UnderSampling(df, 200, \"\")\n",
    "    \n",
    "    #学習データ，検証データに分類\n",
    "    train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    #目的変数設定\n",
    "    #AssyとSAで替えること\n",
    "    obj_cols = [\"defective_cat_16\"] #subassy\n",
    "    #obj_cols = [\"blot_area%s_33\" %r for r in [\"A\", \"B\"]]\n",
    "    \n",
    "    X_train = train_set.drop(columns=obj_cols)\n",
    "    X_test  = test_set.drop(columns=obj_cols)\n",
    "    y_train = train_set[obj_cols[0]].copy()\n",
    "    y_test  = test_set[obj_cols[0]].copy()\n",
    "    exp_col = X_train.columns\n",
    "    \n",
    "    #説明変数の標準化\n",
    "    X_train = DataScaling(X_train)\n",
    "    X_test  = DataScaling(X_test)\n",
    "    \n",
    "    #分類器毎の実力\n",
    "    for flg in clf_method:\n",
    "    \n",
    "        #分類器選択\n",
    "        if flg == \"SGD\":\n",
    "            clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)            \n",
    "        #elif flg == \"Kernel\":\n",
    "            #rbf = RBFSampler(gamma=1, n_components=100, random_state=42)\n",
    "            #X_rbf = rbf.fit_transform(X_train)\n",
    "            #clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)            \n",
    "        elif flg == \"LinearSVC\":\n",
    "            clf = LinearSVC(C=1.0, class_weight=\"balanced\", random_state=42)            \n",
    "        elif flg == \"KNeighbor\":\n",
    "            clf = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")        \n",
    "        #elif flg == \"SVC\":\n",
    "            #clf = SVC(kernel=\"rbf\", C=1.0, class_weight=\"balanced\", random_state=42)        \n",
    "        elif flg == \"RandomForest\":\n",
    "            clf = RandomForestClassifier(criterion=\"entropy\", n_estimators=10, random_state=42, class_weight=\"balanced\")\n",
    "        else:\n",
    "            print(\"No such a method\")\n",
    "        \"\"\"\n",
    "        #学習&評価\n",
    "        if flg == \"Kernel\":\n",
    "            pass\n",
    "            #clf.fit(X_rbf, y_train)\n",
    "            #y_train_predict = clf.predict(rbf.fit_transform(X_train))\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_train_predict = clf.predict(X_train)\n",
    "        \n",
    "        method.append(flg)\n",
    "        precision.append(precision_score(y_train, y_train_predict))\n",
    "        recall.append(recall_score(y_train, y_train_predict))\n",
    "        f1.append(f1_score(y_train, y_train_predict))\n",
    "        conf.append(confusion_matrix(y_train, y_train_predict).ravel())\n",
    "        \n",
    "        print(flg, \"is done\")\n",
    "        \"\"\"\n",
    "\n",
    "        #学習&評価\n",
    "        if flg == \"Kernel\":\n",
    "            pass\n",
    "            #clf.fit(X_rbf, y_train)\n",
    "            #y_test_predict = clf.predict(rbf.fit_transform(X_test))\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_test_predict = clf.predict(X_test)\n",
    "        \n",
    "        #適合率と再現率のプロット\n",
    "        #CalcPrecisionRecall_vs_Threshold(clf, X_train, y_train)\n",
    "\n",
    "        method.append(flg)\n",
    "        precision.append(precision_score(y_test, y_test_predict))\n",
    "        recall.append(recall_score(y_test, y_test_predict))\n",
    "        f1.append(f1_score(y_test, y_test_predict))\n",
    "        conf.append(confusion_matrix(y_test, y_test_predict).ravel())\n",
    "        \n",
    "        print(flg, \"is done\")\n",
    "\n",
    "\n",
    "    #書き出し\n",
    "    confusion = pd.DataFrame(conf, columns = conf_cols)\n",
    "    result[\"method\"] = method\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"f1\"] = f1\n",
    "    \n",
    "    result = pd.concat([result, confusion], axis=1)\n",
    "    \n",
    "    result.to_csv(os.path.join(w_path, \"result_OK_\" + csv_file), index=False)\n",
    "    #result.to_csv(os.path.join(w_path, \"result_\"+csv_file), index=False)\n",
    "\n",
    "    \"\"\"\n",
    "    #モデル評価\n",
    "    y_test_predict = reg.predict(X_test)\n",
    "    y_test_predict = cross_val_predict(clf, X_train, y_train, cv=10, method=\"predict_proba\") #RandomForest\"の場合\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcMethodComparison(csv_file):\n",
    "    \n",
    "    #定義\n",
    "    result = pd.DataFrame()\n",
    "    clf_method = [\"SGD\", \"Kernel\", \"LinearSVC\", \"KNeighbor\", \"SVC\", \"RandomForest\"]\n",
    "    conf_cols  = [\"TN\", \"FP\", \"FN\", \"TP\"]\n",
    "    method, precision, recall, f1, conf = [], [], [], [], []\n",
    "    \n",
    "    #データ準備\n",
    "    df = PrepareFittingData(csv_file)\n",
    "    \n",
    "    #データ数のバランス調整\n",
    "    #df = UnderSampling(df, 200, \"\")\n",
    "    \n",
    "    #学習データ，検証データに分類\n",
    "    train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    #目的変数設定\n",
    "    obj_cols = [\"defective_cat_37\"] #ゲル\n",
    "    \n",
    "    X_train = train_set.drop(columns=obj_cols)\n",
    "    X_test  = test_set.drop(columns=obj_cols)\n",
    "    y_train = train_set[obj_cols[0]].copy()\n",
    "    y_test  = test_set[obj_cols[0]].copy()\n",
    "    exp_col = X_train.columns\n",
    "    \n",
    "    #説明変数の標準化\n",
    "    X_train = DataScaling(X_train)\n",
    "    X_test  = DataScaling(X_test)\n",
    "    \n",
    "    #分類器毎の実力\n",
    "    for flg in clf_method:\n",
    "    \n",
    "        #分類器選択\n",
    "        if flg == \"SGD\":\n",
    "            clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)            \n",
    "        #elif flg == \"Kernel\":\n",
    "            #rbf = RBFSampler(gamma=1, n_components=100, random_state=42)\n",
    "            #X_rbf = rbf.fit_transform(X_train)\n",
    "            #clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)            \n",
    "        elif flg == \"LinearSVC\":\n",
    "            clf = LinearSVC(C=1.0, class_weight=\"balanced\", random_state=42)            \n",
    "        elif flg == \"KNeighbor\":\n",
    "            clf = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")        \n",
    "        #elif flg == \"SVC\":\n",
    "            #clf = SVC(kernel=\"rbf\", C=1.0, class_weight=\"balanced\", random_state=42)        \n",
    "        elif flg == \"RandomForest\":\n",
    "            clf = RandomForestClassifier(criterion=\"entropy\", n_estimators=10, random_state=42, class_weight=\"balanced\")\n",
    "        else:\n",
    "            print(\"No such a method\")\n",
    "        \"\"\"\n",
    "        #学習&評価\n",
    "        if flg == \"Kernel\":\n",
    "            pass\n",
    "            #clf.fit(X_rbf, y_train)\n",
    "            #y_train_predict = clf.predict(rbf.fit_transform(X_train))\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_train_predict = clf.predict(X_train)\n",
    "        \n",
    "        method.append(flg)\n",
    "        precision.append(precision_score(y_train, y_train_predict))\n",
    "        recall.append(recall_score(y_train, y_train_predict))\n",
    "        f1.append(f1_score(y_train, y_train_predict))\n",
    "        conf.append(confusion_matrix(y_train, y_train_predict).ravel())\n",
    "        \n",
    "        print(flg, \"is done\")\n",
    "        \"\"\"\n",
    "\n",
    "        #学習&評価\n",
    "        if flg == \"Kernel\":\n",
    "            pass\n",
    "            #clf.fit(X_rbf, y_train)\n",
    "            #y_test_predict = clf.predict(rbf.fit_transform(X_test))\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_test_predict = clf.predict(X_test)\n",
    "        \n",
    "        #適合率と再現率のプロット\n",
    "        #CalcPrecisionRecall_vs_Threshold(clf, X_train, y_train)\n",
    "\n",
    "        method.append(flg)\n",
    "        precision.append(precision_score(y_test, y_test_predict))\n",
    "        recall.append(recall_score(y_test, y_test_predict))\n",
    "        f1.append(f1_score(y_test, y_test_predict))\n",
    "        conf.append(confusion_matrix(y_test, y_test_predict).ravel())\n",
    "        \n",
    "        print(flg, \"is done\")\n",
    "\n",
    "\n",
    "    #書き出し\n",
    "    confusion = pd.DataFrame(conf, columns = conf_cols)\n",
    "    result[\"method\"] = method\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"f1\"] = f1\n",
    "    \n",
    "    result = pd.concat([result, confusion], axis=1)\n",
    "    \n",
    "    result.to_csv(os.path.join(w_path, \"result_\" + csv_file), index=False)\n",
    "    #result.to_csv(os.path.join(w_path, \"result_\"+csv_file), index=False)\n",
    "\n",
    "    \"\"\"\n",
    "    #モデル評価\n",
    "    y_test_predict = reg.predict(X_test)\n",
    "    y_test_predict = cross_val_predict(clf, X_train, y_train, cv=10, method=\"predict_proba\") #RandomForest\"の場合\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataScaling(X):\n",
    "    scaler = StandardScaler() #scaler = MinMaxScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcPrecisionRecall_vs_Threshold(clf, X, y):\n",
    "    y_scores = cross_val_predict(clf, X, y, cv=10, method=\"decision_function\")\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\n",
    "    PlotPrecisionRecall_vs_Threshold(precisions, recalls, thresholds)\n",
    "    SaveFig(flg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPrecisionRecall_vs_Threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossValidation(reg, X, y, columns):\n",
    "    y_predict = reg.predict(X)\n",
    "    reg_mse = mean_squared_error(y, y_predict)\n",
    "    reg_rmse = np.sqrt(reg_mse)\n",
    "    reg_scores = cross_val_score(reg, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    result = np.sqrt(-reg_scores)\n",
    "\n",
    "    f1 = pd.DataFrame({\"Name\":columns, \"Coefficients\":reg.coef_}).sort_values(by='Coefficients', ascending=False)\n",
    "    f2 = f1.loc[:, [\"Name\", \"Coefficients\"]]\n",
    "    \n",
    "    f2.to_csv(os.path.join(w_folder, csv_file), index=False)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRFCFeatures(csv_file):\n",
    "    \n",
    "    #データ準備\n",
    "    df = PrepareFittingData(csv_file)\n",
    "    \n",
    "    #学習データ，検証データに分類\n",
    "    train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    #目的変数設定\n",
    "    obj_cols = [\"defective_cat_37\"]\n",
    "    \n",
    "    X_train = train_set.drop(columns=obj_cols)\n",
    "    X_test  = test_set.drop(columns=obj_cols)\n",
    "    y_train = train_set[obj_cols[0]].copy()\n",
    "    y_test  = test_set[obj_cols[0]].copy()\n",
    "    exp_col = X_train.columns\n",
    "    \n",
    "    #説明変数の標準化\n",
    "    X_train = DataScaling(X_train)\n",
    "    X_test  = DataScaling(X_test)\n",
    "    \n",
    "    #RaodomForestの学習\n",
    "    clf = RandomForestClassifier(criterion=\"entropy\", n_estimators=10, random_state=42, class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_predict = clf.predict(X_test)\n",
    "\n",
    "    #特徴量の重要度\n",
    "    feature = clf.feature_importances_\n",
    "\n",
    "    #特徴量の重要度順\n",
    "    #indices = np.argsort(feature)[::-1]\n",
    "    indices = np.argsort(feature)\n",
    "\n",
    "    #書き出し\n",
    "    result = pd.DataFrame({\"Name\":exp_col, \"Feature\":feature}).sort_values(by=\"Feature\", ascending=False)\n",
    "    result = result[[\"Name\", \"Feature\"]]\n",
    "    result.to_csv(os.path.join(w_path, \"feature_\"+csv_file), index=False)\n",
    "\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.barh(range(len(feature)),feature[indices], color=\"gray\", align=\"center\")\n",
    "    plt.yticks(range(len(feature)), exp_col[indices], rotation=0)\n",
    "    plt.ylim([-1, len(feature)])\n",
    "    plt.tight_layout()\n",
    "    SaveFig(csv_file[:-4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotHistgramInterval(csv_file):\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    \n",
    "    #OK, NGデータ抽出\n",
    "    df_okay = df[df[\"defective_cat_16\"]==0]\n",
    "    df_blob = df[df[\"defective_cat_16\"]==31]\n",
    "    \n",
    "    df_okay = df_okay.dropna()\n",
    "    df_blob = df_blob.dropna()\n",
    "    \n",
    "    #xmax = max([df_okay[\"cure_time[s]\"].max(), df_blob[\"cure_time[s]\"].max()]) + 100\n",
    "    plot_cols = [\"ti_14_15[s]\", \"cure_time[s]\", \"ti_13_14[s]\", \"ti_15_16[s]\", \"ti_12_13[s]\", \"ti_11_12[s]\"]\n",
    "    #plot_cols = [\"ti_14_15[s]\", \"ti_14_16[s]\", \"ti_13_15[s]\", \"ti_15_16[s]\", \"ti_12_15[s]\"]\n",
    "    \n",
    "    #NGcsvファイル全体をプロット\n",
    "    #1画像辺りのサブプロット数\n",
    "    plt_row = 6\n",
    "    plt_col = 1\n",
    "    \n",
    "    #figNo設定\n",
    "    i_mat = 0\n",
    "    i_fig = 0\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    for col in plot_cols:\n",
    "        i_mat += 1\n",
    "        \n",
    "        plt.subplot(plt_row, plt_col, i_mat)\n",
    "        \n",
    "        if not df_okay.empty:\n",
    "            plt.hist(df_okay[col].values, bins=50, alpha=0.3, histtype=\"stepfilled\", color=\"b\", label=\"OK\")\n",
    "        if not df_blob.empty:\n",
    "            plt.hist(df_blob[col].values, bins=50, alpha=0.3, histtype=\"stepfilled\", color=\"r\", label=\"NG\")\n",
    "        \n",
    "        plt.title(col)\n",
    "        plt.tick_params(labelsize=8)\n",
    "        plt.ylim([0, 2000])\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    SaveFig(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■■■■■　MainProgram　■■■■■"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義\n",
    "r_path = os.path.join(\"..\", \"AssemblyData\", \"Extract\")\n",
    "w_path = os.path.join(\"..\", \"AssemblyData_Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データファイル取得\n",
    "#csv_files = GetIntervalCSVFiles(r_path)\n",
    "#csv_files = GetConcatSACSVFiles(r_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: DtypeWarning: Columns (13,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD is done\n",
      "No such a method\n",
      "Kernel is done\n",
      "LinearSVC is done\n",
      "KNeighbor is done\n",
      "No such a method\n",
      "SVC is done\n",
      "RandomForest is done\n"
     ]
    }
   ],
   "source": [
    "#SubAssyインターバル学習\n",
    "csv_files = GetConcatSubAssyFiles(r_path)\n",
    "for csv_file in csv_files:\n",
    "    CalcMethodComparison(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD is done\n",
      "No such a method\n",
      "Kernel is done\n",
      "LinearSVC is done\n",
      "KNeighbor is done\n",
      "No such a method\n",
      "SVC is done\n",
      "RandomForest is done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Assyゲル気泡学習\n",
    "csv_files = GetConcatAssyWithGelFiles(r_path)\n",
    "for csv_file in csv_files:\n",
    "    CalcMethodComparison(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetRFCFeatures(csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ヒストグラム\n",
    "for csv_file in csv_files:\n",
    "    PlotHistgramInterval(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-b1e59ef1f61f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcsv_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetConcatAssyFiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mCalcMethodComparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-90e92279d99a>\u001b[0m in \u001b[0;36mCalcMethodComparison\u001b[1;34m(csv_file)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0my_test_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m#適合率と再現率のプロット\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m--> 308\u001b[1;33m                                  dense_output=True) + self.intercept_\n\u001b[0m\u001b[0;32m    309\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Assyインターバル学習\n",
    "csv_files = GetConcatAssyFiles(r_path)\n",
    "for csv_file in csv_files:\n",
    "    CalcMethodComparison(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■■■■■　確認　■■■■■"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■■■■■　バックアップ　■■■■■"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#工程間　単体\n",
    "def PrepareFittingData(csv_file):\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    \n",
    "    #OK:0, NG:31のデータ抽出 query は　boolインデックスより新しいみたい\n",
    "    df = df.query(\"defective_cat_16 == 0 or defective_cat_16 == 31\") \n",
    "    \n",
    "    #X, Yとなるデータの抽出\n",
    "    cols = [\"ti_%s_%s[s]\" %(i, i+1) for i in np.arange(11,16)]\n",
    "    cols.extend([\"cure_time[s]\", \"defective_cat_16\"])\n",
    "    df = df[cols]\n",
    "    \n",
    "    #NG:31を1に変換\n",
    "    df.loc[df[\"defective_cat_16\"]==31, \"defective_cat_16\"] = 1\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#工程間　相互\n",
    "def PrepareFittingData(csv_file):\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    \n",
    "    #OK:0, NG:31のデータ抽出 query は　boolインデックスより新しいみたい\n",
    "    df = df.query(\"defective_cat_16 == 0 or defective_cat_16 == 31\") \n",
    "    \n",
    "    #X, Yとなるデータの抽出\n",
    "    cols = df.columns[df.columns.str.startswith(\"ti_\")].tolist()\n",
    "    cols.extend([\"cure_time[s]\", \"defective_cat_16\"])\n",
    "    df = df[cols]\n",
    "    \n",
    "    #NG:31を1に変換\n",
    "    df.loc[df[\"defective_cat_16\"]==31, \"defective_cat_16\"] = 1\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
