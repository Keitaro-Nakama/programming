{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#0,1問題は分類に当たる\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■■■■■　関数　■■■■■"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveFig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=600):\n",
    "   \n",
    "    images_path = os.path.join(\"..\", \"Images\")\n",
    "    os.makedirs(images_path, exist_ok=True)\n",
    "    file_name = os.path.join(images_path, fig_id + \".\" + fig_extension)\n",
    "    \n",
    "    #print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(file_name, format=fig_extension, dpi=resolution)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetConcatSubAssyFiles(r_path):\n",
    "    files = os.listdir(r_path)\n",
    "    csv_files = []\n",
    "    \n",
    "    for file in files:\n",
    "        if(file.find(\".csv\") != -1 and file.upper().find(\"CONCAT_SA\") != -1):\n",
    "            csv_files.append(file)\n",
    "            \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetConcatAssyFiles(r_path):\n",
    "    files = os.listdir(r_path)\n",
    "    csv_files = []\n",
    "    \n",
    "    for file in files:\n",
    "        if (file.find(\".csv\") != -1 and file.upper().startswith(\"CONCAT_ASSY\") != -1):\n",
    "            csv_files.append(file)\n",
    "            \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnderSampling(X,num,label) :\n",
    "\n",
    "    # KMeansによるクラスタリング\n",
    "    km = KMeans(random_state=42)\n",
    "    km.fit(X,Y)\n",
    "    X[\"Cluster\"] = km.predict(X)\n",
    "\n",
    "    # 群別の構成比を少数派の件数に乗じて群別の抽出件数を計算\n",
    "    count_sum = X.groupby(\"Cluster\").count().iloc[0:,0].as_matrix()\n",
    "    ratio = count_sum / count_sum.sum()\n",
    "    samp_num = np.round(ratio * num,0).astype(np.int32)\n",
    "\n",
    "    # 群別にサンプリング処理を実施\n",
    "    for i in np.arange(8) :\n",
    "        tmp = X[X[\"Cluster\"]==i]\n",
    "        if i == 0 :\n",
    "            tmp1 = X.sample(samp_num[i],replace=True)\n",
    "        else :\n",
    "            tmp2 = X.sample(samp_num[i],replace=True)\n",
    "            tmp1 = pd.concat([tmp1,tmp2])\n",
    "    tmp1[\"Class\"] = label\n",
    "    \n",
    "    return tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResetIndex(df):\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# ベタ書き #\n",
    "############\n",
    "def Main(csv_file):\n",
    "    \n",
    "    #ランダムサンプル数 50000だと死んだ\n",
    "    n_samples = 20000\n",
    "    \n",
    "    #列名定義\n",
    "    num_cols = [\"ti_%s_to_%s[s]\" %(i, i+1) for i in np.arange(11,16)] + [\"cure_time[s]\"]\n",
    "    cat_cols = [\"storage_loc_14\", \"sub1\"]\n",
    "    #cat_cols = [\"sa_lot\", \"storage_loc_14\", \"sub1\", \"film_lot\"]\n",
    "    obj_cols = [\"defective_cat_16\"]\n",
    "    \n",
    "    #工程No定義\n",
    "    pro_no = np.arange(11, 17)\n",
    "    \n",
    "    #結果格納系定義\n",
    "    result = pd.DataFrame()\n",
    "    clf_method = [\"SGD\", \"Kernel\", \"LinearSVC\", \"KNeighbor\", \"SVC\", \"RandomForest\"]\n",
    "    conf_cols  = [\"TN\", \"FP\", \"FN\", \"TP\"]\n",
    "    method, precision, recall, f1, conf = [], [], [], [], []\n",
    "    \n",
    "    #データの型設定→メモリ容量削減のため\n",
    "    dtypes = {\"sa_lot\": \"int32\", \"sa_seri\": \"int32\", \"storage_loc_14\": \"int16\", \"defective_cat_16\": \"int8\"}\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    #df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    reader = pd.read_csv(os.path.join(r_path, csv_file), dtype=dtypes, low_memory=False, chunksize=2000)\n",
    "    df = pd.concat((r for r in reader), ignore_index=True)\n",
    "    \n",
    "    for (i, j) in zip(pro_no[:-1], pro_no[1:]):\n",
    "        df[\"ti_%s_to_%s[s]\" %(i, j)] = df[\"ti_%s_to_%s[s]\" %(i, j)].astype(\"int32\")\n",
    "    df[\"cure_time[s]\"] = df[\"cure_time[s]\"].astype(\"int32\")\n",
    "\n",
    "    #OK:0, NG:31のデータ抽出 queryは新しい手法みたい．外海さん正解！\n",
    "    df = df.query(\"defective_cat_16 == 0 or defective_cat_16 == 31\") \n",
    "    df.loc[df[\"defective_cat_16\"]==31, \"defective_cat_16\"] = 1\n",
    "    df = df.dropna(subset=[\"sub1\"])\n",
    "    df = df.sample(n=n_samples)\n",
    "    df = ResetIndex(df)\n",
    "    #df = df.reset_index()\n",
    "    #df = df.drop(\"index\", axis=1)\n",
    "    \n",
    "    #学習データ，検証データに分割\n",
    "    train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    #説明変数，目的変数に分割\n",
    "    X_num_train = train_set[num_cols]\n",
    "    X_cat_train = train_set[cat_cols]\n",
    "    X_num_test  = test_set[num_cols]\n",
    "    X_cat_test  = test_set[cat_cols]\n",
    "    y_train     = train_set[obj_cols[0]].copy()\n",
    "    y_test      = test_set[obj_cols[0]].copy() \n",
    "    \n",
    "    #スケーリング後の再columns設定のため抜き出し\n",
    "    sorted_num_cols = X_num_train.columns\n",
    "    \n",
    "    #数値データをスケーリング\n",
    "    X_num_train = DataScaling(X_num_train)\n",
    "    X_num_test  = DataScaling(X_num_test)\n",
    "    \n",
    "    #スケーリングはndarrayで返されるので，Concat様にcolumnsを付ける\n",
    "    X_num_train = pd.DataFrame(X_num_train, index=None, columns=sorted_num_cols)\n",
    "    X_num_test = pd.DataFrame(X_num_test, index=None, columns=sorted_num_cols)\n",
    "    \n",
    "    #カテゴリデータをone-hot-encode\n",
    "    X_cat_train = pd.get_dummies(X_cat_train, columns=cat_cols)\n",
    "    X_cat_test  = pd.get_dummies(X_cat_test, columns=cat_cols)\n",
    "    \n",
    "    #indexを再度\n",
    "    X_cat_train = ResetIndex(X_cat_train)\n",
    "    X_cat_test  = ResetIndex(X_cat_test) \n",
    "    \n",
    "    #数値とカテゴリ値を結合\n",
    "    X_train = pd.concat([X_num_train, X_cat_train], axis=1)\n",
    "    X_test  = pd.concat([X_num_test, X_cat_test], axis=1)\n",
    "\n",
    "    #分類器毎の実力\n",
    "    for flg in clf_method:\n",
    "    \n",
    "        #分類器選択\n",
    "        if flg == \"SGD\":\n",
    "            clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)            \n",
    "        elif flg == \"Kernel\":\n",
    "            rbf = RBFSampler(gamma=1, n_components=100, random_state=42)\n",
    "            X_rbf = rbf.fit_transform(X_train)\n",
    "            clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)            \n",
    "        elif flg == \"LinearSVC\":\n",
    "            clf = LinearSVC(C=1.0, class_weight=\"balanced\", random_state=42)            \n",
    "        elif flg == \"KNeighbor\":\n",
    "            clf = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")        \n",
    "        #elif flg == \"SVC\":\n",
    "            #clf = SVC(kernel=\"rbf\", C=1.0, class_weight=\"balanced\", random_state=42)        \n",
    "        elif flg == \"RandomForest\":\n",
    "            clf = RandomForestClassifier(criterion=\"entropy\", n_estimators=10, random_state=42, class_weight=\"balanced\")\n",
    "        else:\n",
    "            print(\"No such a method\")\n",
    "\n",
    "        #学習&評価\n",
    "        #trainセットバージョン\n",
    "        print(flg, \"is being learned\")\n",
    "        if flg == \"Kernel\":\n",
    "            clf.fit(X_rbf, y_train.ravel())\n",
    "            y_train_predict = clf.predict(rbf.fit_transform(X_train))\n",
    "        else:\n",
    "            clf.fit(X_train, y_train.ravel())\n",
    "            y_train_predict = clf.predict(X_train)\n",
    "        \n",
    "        #適合率と再現率のプロット\n",
    "        #CalcPrecisionRecall_vs_Threshold(clf, X_train, y_train)\n",
    "\n",
    "        method.append(flg)\n",
    "        precision.append(precision_score(y_train, y_train_predict))\n",
    "        recall.append(recall_score(y_train, y_train_predict))\n",
    "        f1.append(f1_score(y_train, y_train_predict))\n",
    "        conf.append(confusion_matrix(y_train, y_train_predict).ravel())\n",
    "        \n",
    "        \"\"\"\n",
    "        #学習&評価\n",
    "        #testセットバージョン\n",
    "        print(flg, \"is being learned\")\n",
    "        if flg == \"Kernel\":\n",
    "            clf.fit(X_rbf, y_train)\n",
    "            y_test_predict = clf.predict(rbf.fit_transform(X_test))\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_test_predict = clf.predict(X_test)\n",
    "        \n",
    "        #適合率と再現率のプロット\n",
    "        #CalcPrecisionRecall_vs_Threshold(clf, X_train, y_train)\n",
    "\n",
    "        method.append(flg)\n",
    "        precision.append(precision_score(y_test, y_test_predict))\n",
    "        recall.append(recall_score(y_test, y_test_predict))\n",
    "        f1.append(f1_score(y_test, y_test_predict))\n",
    "        conf.append(confusion_matrix(y_test, y_test_predict).ravel())\n",
    "        \"\"\"\n",
    "        \n",
    "        print(flg, \"is done\")\n",
    "\n",
    "    #書き出し\n",
    "    confusion = pd.DataFrame(conf, columns = conf_cols)\n",
    "    result[\"method\"] = method\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"f1\"] = f1\n",
    "    \n",
    "    result = pd.concat([result, confusion], axis=1)\n",
    "    \n",
    "    result.to_csv(os.path.join(w_path, \"result_no_sa_lot_\"+csv_file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRFCFeatures(csv_file):\n",
    "    \n",
    "    #ランダムサンプル数 50000だと死んだ\n",
    "    n_samples = 20000\n",
    "    \n",
    "    #列名定義\n",
    "    num_cols = [\"ti_%s_to_%s[s]\" %(i, i+1) for i in np.arange(11,16)] + [\"cure_time[s]\"]\n",
    "    cat_cols = [\"storage_loc_14\", \"sub1\"]\n",
    "    #cat_cols = [\"sa_lot\", \"storage_loc_14\", \"sub1\", \"film_lot\"]\n",
    "    obj_cols = [\"defective_cat_16\"]\n",
    "    \n",
    "    #工程No定義\n",
    "    pro_no = np.arange(11, 17)\n",
    "    \n",
    "    #結果格納系定義\n",
    "    result = pd.DataFrame()\n",
    "    clf_method = [\"SGD\", \"Kernel\", \"LinearSVC\", \"KNeighbor\", \"SVC\", \"RandomForest\"]\n",
    "    conf_cols  = [\"TN\", \"FP\", \"FN\", \"TP\"]\n",
    "    method, precision, recall, f1, conf = [], [], [], [], []\n",
    "    \n",
    "    #データの型設定→メモリ容量削減のため\n",
    "    dtypes = {\"sa_lot\": \"int32\", \"sa_seri\": \"int32\", \"storage_loc_14\": \"int16\", \"defective_cat_16\": \"int8\"}\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    #df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    reader = pd.read_csv(os.path.join(r_path, csv_file), dtype=dtypes, low_memory=False, chunksize=2000)\n",
    "    df = pd.concat((r for r in reader), ignore_index=True)\n",
    "    \n",
    "    for (i, j) in zip(pro_no[:-1], pro_no[1:]):\n",
    "        df[\"ti_%s_to_%s[s]\" %(i, j)] = df[\"ti_%s_to_%s[s]\" %(i, j)].astype(\"int32\")\n",
    "    df[\"cure_time[s]\"] = df[\"cure_time[s]\"].astype(\"int32\")\n",
    "\n",
    "    #OK:0, NG:31のデータ抽出 queryは新しい手法みたい．外海さん正解！\n",
    "    df = df.query(\"defective_cat_16 == 0 or defective_cat_16 == 31\") \n",
    "    df.loc[df[\"defective_cat_16\"]==31, \"defective_cat_16\"] = 1\n",
    "    df = df.dropna(subset=[\"sub1\"])\n",
    "    df = df.sample(n=n_samples)\n",
    "    df = ResetIndex(df)\n",
    "    #df = df.reset_index()\n",
    "    #df = df.drop(\"index\", axis=1)\n",
    "    \n",
    "    #学習データ，検証データに分割\n",
    "    train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    #説明変数，目的変数に分割\n",
    "    X_num_train = train_set[num_cols]\n",
    "    X_cat_train = train_set[cat_cols]\n",
    "    X_num_test  = test_set[num_cols]\n",
    "    X_cat_test  = test_set[cat_cols]\n",
    "    y_train     = train_set[obj_cols[0]].copy()\n",
    "    y_test      = test_set[obj_cols[0]].copy() \n",
    "    \n",
    "    #スケーリング後の再columns設定のため抜き出し\n",
    "    sorted_num_cols = X_num_train.columns\n",
    "    \n",
    "    #数値データをスケーリング\n",
    "    X_num_train = DataScaling(X_num_train)\n",
    "    X_num_test  = DataScaling(X_num_test)\n",
    "    \n",
    "    #スケーリングはndarrayで返されるので，Concat様にcolumnsを付ける\n",
    "    X_num_train = pd.DataFrame(X_num_train, index=None, columns=sorted_num_cols)\n",
    "    X_num_test = pd.DataFrame(X_num_test, index=None, columns=sorted_num_cols)\n",
    "    \n",
    "    #カテゴリデータをone-hot-encode\n",
    "    X_cat_train = pd.get_dummies(X_cat_train, columns=cat_cols)\n",
    "    X_cat_test  = pd.get_dummies(X_cat_test, columns=cat_cols)\n",
    "    \n",
    "    #indexを再度\n",
    "    X_cat_train = ResetIndex(X_cat_train)\n",
    "    X_cat_test  = ResetIndex(X_cat_test) \n",
    "    \n",
    "    #数値とカテゴリ値を結合\n",
    "    X_train = pd.concat([X_num_train, X_cat_train], axis=1)\n",
    "    X_test  = pd.concat([X_num_test, X_cat_test], axis=1)\n",
    "    \n",
    "    #列名取得\n",
    "    exp_col = X_train.columns\n",
    "    \n",
    "    #RaodomForestの学習\n",
    "    clf = RandomForestClassifier(criterion=\"entropy\", n_estimators=10, random_state=42, class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    y_train_predict = clf.predict(X_train)\n",
    "\n",
    "    #特徴量の重要度\n",
    "    feature = clf.feature_importances_\n",
    "\n",
    "    #特徴量の重要度順\n",
    "    #indices = np.argsort(feature)[::-1]\n",
    "    indices = np.argsort(feature)\n",
    "\n",
    "    #書き出し\n",
    "    result = pd.DataFrame({\"Name\":exp_col, \"Feature\":feature}).sort_values(by=\"Feature\", ascending=False)\n",
    "    result = result[[\"Name\", \"Feature\"]]\n",
    "    #result.to_csv(os.path.join(w_path, \"feature_without_salot_\"+csv_file), index=False)\n",
    "\n",
    "    n = 10\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.barh(range(n),feature[indices[-n:]], color=\"gray\", align=\"center\")\n",
    "    plt.yticks(range(n), exp_col[indices[-n:]], rotation=0, fontsize=7)\n",
    "    plt.ylim([-1, n])\n",
    "    #plt.barh(range(len(feature)),feature[indices], color=\"gray\", align=\"center\")\n",
    "    #plt.yticks(range(len(feature)), exp_col[indices], rotation=0, fontsize=7)\n",
    "    #plt.ylim([-1, len(feature)])\n",
    "    plt.tight_layout()\n",
    "    SaveFig(csv_file[:-4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD is being learned\n",
      "SGD is done\n",
      "Kernel is being learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is done\n",
      "LinearSVC is being learned\n",
      "LinearSVC is done\n",
      "KNeighbor is being learned\n",
      "KNeighbor is done\n",
      "No such a method\n",
      "SVC is being learned\n",
      "SVC is done\n",
      "RandomForest is being learned\n",
      "RandomForest is done\n"
     ]
    }
   ],
   "source": [
    "csv_files = GetConcatSubAssyFiles(r_path)\n",
    "Main(csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10001165234\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "GetRFCFeatures(csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcMethodComparison(csv_file):\n",
    "    \n",
    "    #定義\n",
    "    result = pd.DataFrame()\n",
    "    clf_method = [\"SGD\", \"Kernel\", \"LinearSVC\", \"KNeighbor\", \"SVC\", \"RandomForest\"]\n",
    "    conf_cols  = [\"TN\", \"FP\", \"FN\", \"TP\"]\n",
    "    method, precision, recall, f1, conf = [], [], [], [], []\n",
    "    \n",
    "    #データ準備\n",
    "    df = PrepareFittingData(csv_file)\n",
    "    \n",
    "    #学習データ，検証データに分類\n",
    "    train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    #目的変数設定\n",
    "    #obj_cols = [\"defective_cat_16\"] #subassy\n",
    "    obj_cols = [\"blot_area%s_33\" %r for r in [\"A\", \"B\"]]\n",
    "    \n",
    "    X_train = train_set.drop(columns=obj_cols)\n",
    "    X_test  = test_set.drop(columns=obj_cols)\n",
    "    y_train = train_set[obj_cols[0]].copy()\n",
    "    y_test  = test_set[obj_cols[0]].copy()\n",
    "    exp_col = X_train.columns\n",
    "    \n",
    "    #説明変数の標準化\n",
    "    X_train = DataScaling(X_train)\n",
    "    X_test  = DataScaling(X_test)\n",
    "    \n",
    "    #分類器毎の実力\n",
    "    for flg in clf_method:\n",
    "    \n",
    "        #分類器選択\n",
    "        if flg == \"SGD\":\n",
    "            clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)            \n",
    "        elif flg == \"Kernel\":\n",
    "            rbf = RBFSampler(gamma=1, n_components=100, random_state=42)\n",
    "            X_rbf = rbf.fit_transform(X_train)\n",
    "            clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)            \n",
    "        elif flg == \"LinearSVC\":\n",
    "            clf = LinearSVC(C=1.0, class_weight=\"balanced\", random_state=42)            \n",
    "        elif flg == \"KNeighbor\":\n",
    "            clf = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")        \n",
    "        elif flg == \"SVC\":\n",
    "            clf = SVC(kernel=\"rbf\", C=1.0, class_weight=\"balanced\", random_state=42)        \n",
    "        elif flg == \"RandomForest\":\n",
    "            clf = RandomForestClassifier(criterion=\"entropy\", n_estimators=10, random_state=42, class_weight=\"balanced\")\n",
    "        else:\n",
    "            print(\"No such a method\")\n",
    "\n",
    "        #学習&評価\n",
    "        if flg == \"Kernel\":\n",
    "            clf.fit(X_rbf, y_train)\n",
    "            y_test_predict = clf.predict(rbf.fit_transform(X_test))\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_test_predict = clf.predict(X_test)\n",
    "        \n",
    "        #適合率と再現率のプロット\n",
    "        #CalcPrecisionRecall_vs_Threshold(clf, X_train, y_train)\n",
    "\n",
    "        method.append(flg)\n",
    "        precision.append(precision_score(y_test, y_test_predict))\n",
    "        recall.append(recall_score(y_test, y_test_predict))\n",
    "        f1.append(f1_score(y_test, y_test_predict))\n",
    "        conf.append(confusion_matrix(y_test, y_test_predict).ravel())\n",
    "\n",
    "    #書き出し\n",
    "    confusion = pd.DataFrame(conf, columns = conf_cols)\n",
    "    result[\"method\"] = method\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"f1\"] = f1\n",
    "    \n",
    "    result = pd.concat([result, confusion], axis=1)\n",
    "    \n",
    "    result.to_csv(os.path.join(w_path, \"result_\"+csv_file), index=False)\n",
    "\n",
    "    \"\"\"\n",
    "    #モデル評価\n",
    "    y_test_predict = reg.predict(X_test)\n",
    "    y_test_predict = cross_val_predict(clf, X_train, y_train, cv=10, method=\"predict_proba\") #RandomForest\"の場合\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataScaling(X):\n",
    "    scaler = StandardScaler() #scaler = MinMaxScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcPrecisionRecall_vs_Threshold(clf, X, y):\n",
    "    y_scores = cross_val_predict(clf, X, y, cv=10, method=\"decision_function\")\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\n",
    "    PlotPrecisionRecall_vs_Threshold(precisions, recalls, thresholds)\n",
    "    SaveFig(flg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPrecisionRecall_vs_Threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossValidation(reg, X, y, columns):\n",
    "    y_predict = reg.predict(X)\n",
    "    reg_mse = mean_squared_error(y, y_predict)\n",
    "    reg_rmse = np.sqrt(reg_mse)\n",
    "    reg_scores = cross_val_score(reg, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    result = np.sqrt(-reg_scores)\n",
    "\n",
    "    f1 = pd.DataFrame({\"Name\":columns, \"Coefficients\":reg.coef_}).sort_values(by='Coefficients', ascending=False)\n",
    "    f2 = f1.loc[:, [\"Name\", \"Coefficients\"]]\n",
    "    \n",
    "    f2.to_csv(os.path.join(w_folder, csv_file), index=False)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotHistgramInterval(csv_file):\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    \n",
    "    #OK, NGデータ抽出\n",
    "    df_okay = df[df[\"defective_cat_16\"]==0]\n",
    "    df_blob = df[df[\"defective_cat_16\"]==31]\n",
    "    \n",
    "    df_okay = df_okay.dropna()\n",
    "    df_blob = df_blob.dropna()\n",
    "    \n",
    "    #xmax = max([df_okay[\"cure_time[s]\"].max(), df_blob[\"cure_time[s]\"].max()]) + 100\n",
    "    plot_cols = [\"ti_14_15[s]\", \"cure_time[s]\", \"ti_13_14[s]\", \"ti_15_16[s]\", \"ti_12_13[s]\", \"ti_11_12[s]\"]\n",
    "    #plot_cols = [\"ti_14_15[s]\", \"ti_14_16[s]\", \"ti_13_15[s]\", \"ti_15_16[s]\", \"ti_12_15[s]\"]\n",
    "    \n",
    "    #NGcsvファイル全体をプロット\n",
    "    #1画像辺りのサブプロット数\n",
    "    plt_row = 6\n",
    "    plt_col = 1\n",
    "    \n",
    "    #figNo設定\n",
    "    i_mat = 0\n",
    "    i_fig = 0\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    \n",
    "    for col in plot_cols:\n",
    "        i_mat += 1\n",
    "        \n",
    "        plt.subplot(plt_row, plt_col, i_mat)\n",
    "        \n",
    "        if not df_okay.empty:\n",
    "            plt.hist(df_okay[col].values, bins=50, alpha=0.3, histtype=\"stepfilled\", color=\"b\", label=\"OK\")\n",
    "        if not df_blob.empty:\n",
    "            plt.hist(df_blob[col].values, bins=50, alpha=0.3, histtype=\"stepfilled\", color=\"r\", label=\"NG\")\n",
    "        \n",
    "        plt.title(col)\n",
    "        plt.tick_params(labelsize=8)\n",
    "        plt.ylim([0, 2000])\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    SaveFig(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■■■■■　MainProgram　■■■■■"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義\n",
    "r_path = os.path.join(\"..\", \"AssemblyData\", \"Extract\")\n",
    "w_path = os.path.join(\"..\", \"AssemblyData_Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データファイル取得\n",
    "#csv_files = GetIntervalCSVFiles(r_path)\n",
    "csv_files = GetConcatSACSVFiles(r_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalcMethodComparison(csv_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetRFCFeatures(csv_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ヒストグラム\n",
    "for csv_file in csv_files:\n",
    "    PlotHistgramInterval(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-b1e59ef1f61f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcsv_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetConcatAssyFiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mCalcMethodComparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-90e92279d99a>\u001b[0m in \u001b[0;36mCalcMethodComparison\u001b[1;34m(csv_file)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0my_test_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m#適合率と再現率のプロット\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m--> 308\u001b[1;33m                                  dense_output=True) + self.intercept_\n\u001b[0m\u001b[0;32m    309\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Assyインターバル学習\n",
    "csv_files = GetConcatAssyFiles(r_path)\n",
    "for csv_file in csv_files:\n",
    "    CalcMethodComparison(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■■■■■　確認　■■■■■"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■■■■■　バックアップ　■■■■■"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#工程間　単体\n",
    "def PrepareFittingData(csv_file):\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    \n",
    "    #OK:0, NG:31のデータ抽出 query は　boolインデックスより新しいみたい\n",
    "    df = df.query(\"defective_cat_16 == 0 or defective_cat_16 == 31\") \n",
    "    \n",
    "    #X, Yとなるデータの抽出\n",
    "    cols = [\"ti_%s_%s[s]\" %(i, i+1) for i in np.arange(11,16)]\n",
    "    cols.extend([\"cure_time[s]\", \"defective_cat_16\"])\n",
    "    df = df[cols]\n",
    "    \n",
    "    #NG:31を1に変換\n",
    "    df.loc[df[\"defective_cat_16\"]==31, \"defective_cat_16\"] = 1\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#工程間　相互\n",
    "def PrepareFittingData(csv_file):\n",
    "    \n",
    "    #IntervalCSV読み込み\n",
    "    df = pd.read_csv(os.path.join(r_path, csv_file))\n",
    "    \n",
    "    #OK:0, NG:31のデータ抽出 query は　boolインデックスより新しいみたい\n",
    "    df = df.query(\"defective_cat_16 == 0 or defective_cat_16 == 31\") \n",
    "    \n",
    "    #X, Yとなるデータの抽出\n",
    "    cols = df.columns[df.columns.str.startswith(\"ti_\")].tolist()\n",
    "    cols.extend([\"cure_time[s]\", \"defective_cat_16\"])\n",
    "    df = df[cols]\n",
    "    \n",
    "    #NG:31を1に変換\n",
    "    df.loc[df[\"defective_cat_16\"]==31, \"defective_cat_16\"] = 1\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def GetRFCFeatures(csv_file):\n",
    "    \n",
    "    #データ準備\n",
    "    df = PrepareFittingData(csv_file)\n",
    "    \n",
    "    #学習データ，検証データに分類\n",
    "    train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    #目的変数設定\n",
    "    obj_cols = [\"defective_cat_16\"]\n",
    "    \n",
    "    X_train = train_set.drop(columns=obj_cols)\n",
    "    X_test  = test_set.drop(columns=obj_cols)\n",
    "    y_train = train_set[obj_cols[0]].copy()\n",
    "    y_test  = test_set[obj_cols[0]].copy()\n",
    "    exp_col = X_train.columns\n",
    "    \n",
    "    #説明変数の標準化\n",
    "    X_train = DataScaling(X_train)\n",
    "    X_test  = DataScaling(X_test)\n",
    "    \n",
    "    #RaodomForestの学習\n",
    "    clf = RandomForestClassifier(criterion=\"entropy\", n_estimators=10, random_state=42, class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_predict = clf.predict(X_test)\n",
    "\n",
    "    #特徴量の重要度\n",
    "    feature = clf.feature_importances_\n",
    "\n",
    "    #特徴量の重要度順\n",
    "    #indices = np.argsort(feature)[::-1]\n",
    "    indices = np.argsort(feature)\n",
    "\n",
    "    #書き出し\n",
    "    result = pd.DataFrame({\"Name\":exp_col, \"Feature\":feature}).sort_values(by=\"Feature\", ascending=False)\n",
    "    result = result[[\"Name\", \"Feature\"]]\n",
    "    result.to_csv(os.path.join(w_path, \"feature_\"+csv_file), index=False)\n",
    "\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.barh(range(len(feature)),feature[indices], color=\"gray\", align=\"center\")\n",
    "    plt.yticks(range(len(feature)), exp_col[indices], rotation=0)\n",
    "    plt.ylim([-1, len(feature)])\n",
    "    plt.tight_layout()\n",
    "    SaveFig(csv_file[:-4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
